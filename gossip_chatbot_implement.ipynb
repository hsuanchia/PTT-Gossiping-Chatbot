{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gossip_chatbot_implement.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "131A5b8eqgYqn9YCPYHdnhald1n9iowoY",
      "authorship_tag": "ABX9TyOjTIkfF4w3pIiiqP6Txy6M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsuanchia/PTT-Gossiping-Chatbot/blob/main/gossip_chatbot_implement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcNR5UbfO4Yg",
        "outputId": "12f61785-89f0-4429-8242-87ded9581cb7"
      },
      "source": [
        "# 因為我使用CKIP，所以每次可能都需要重新下載一次，很快\n",
        "! pip install ckiptagger[tagger]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ckiptagger[tagger]\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/bc/5cbf8d019167d5e5e1775069fb8b71a08691ab847e2926bbe7dee9a19010/ckiptagger-0.2.1-py3-none-any.whl\n",
            "\u001b[33m  WARNING: ckiptagger 0.2.1 does not provide the extra 'tagger'\u001b[0m\n",
            "Installing collected packages: ckiptagger\n",
            "Successfully installed ckiptagger-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U27vQ0hQKOo9"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from ckiptagger import WS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd7KBOfFYBy8"
      },
      "source": [
        "#Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfk9uWhkZsjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "ece8d170-45dd-4565-cb6b-ae99109f9523"
      },
      "source": [
        "model_path = '/content/drive/MyDrive/NLP/Gossip_chatbot_9.h5'\n",
        "model = load_model(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f65eece0c3c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/NLP/Gossip_chatbot_9.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   raise IOError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n\u001b[0;32m--> 116\u001b[0;31m          constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /content/drive/MyDrive/NLP/Gossip_chatbot_9.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HMBTag9KkxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81f77fc-8b44-4933-cd0a-f7d38420da17"
      },
      "source": [
        "ws = WS(\"/content/drive/MyDrive/data\") # CKIP的斷詞套件，如果沒有可能需要另外下載\n",
        "\n",
        "Q_word_to_index = pickle.load(open(f'/content/drive/MyDrive/NLP/Q_word-index.pkl', 'rb'))\n",
        "Q_index_to_word = pickle.load(open(f'/content/drive/MyDrive/NLP/Q_index-word.pkl', 'rb'))\n",
        "A_word_to_index = pickle.load(open(f'/content/drive/MyDrive/NLP/A_word-index.pkl', 'rb'))\n",
        "A_index_to_word = pickle.load(open(f'/content/drive/MyDrive/NLP/A_index-word.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:909: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEHquk9ZYKDd"
      },
      "source": [
        "#Initialization parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rISQ7qE_Lzju"
      },
      "source": [
        "start = '<sos>'\n",
        "end = '<end>' \n",
        "unk = '<unk>'\n",
        "pad = '<pad>'\n",
        "segment_delimiter = {'?', '？', '!', '！', ' ', '。', ',', ',','，', ';', ':', '、', '.', '(', ')', '「', '」', '（', '）'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41m1FqP7Lmzd"
      },
      "source": [
        "Q_word_to_index[start] = 31652\n",
        "Q_word_to_index[end] = 31653\n",
        "Q_word_to_index[unk] = 31654\n",
        "Q_index_to_word[31652] = start\n",
        "Q_index_to_word[31653] = end\n",
        "Q_index_to_word[31654] = unk\n",
        "\n",
        "A_word_to_index[start] = 36397\n",
        "A_word_to_index[end] = 36398\n",
        "A_word_to_index[unk] = 36399\n",
        "A_index_to_word[36397] = start\n",
        "A_index_to_word[36398] = end\n",
        "A_index_to_word[36399] = unk\n",
        "\n",
        "Q_max_length = 30 #q_data max = 30, a_data max = 59\n",
        "A_max_length = 60\n",
        "Q_vocsize = len(Q_word_to_index)\n",
        "A_vocsize = len(A_word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZm3idrm_JjZ"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iEE57lQPAhv"
      },
      "source": [
        "def splitsentence(sentence):\n",
        "  return ws(sentence_list=[sentence],sentence_segmentation = True, segment_delimiter_set=segment_delimiter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPG0emCwNcpY"
      },
      "source": [
        "#將文字轉成integer sequence\n",
        "def fit_sentence(sentence,word2index,max_length): \n",
        "  ans = []\n",
        "  for i in sentence:\n",
        "    if i in word2index.keys():\n",
        "      ans.append(word2index[i])\n",
        "    else:\n",
        "      ans.append(word2index[unk])\n",
        "  seq = pad_sequences([ans],maxlen=max_length,padding='post')\n",
        "  return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88uDfW_oYQng"
      },
      "source": [
        "# Chat Bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8i1Qw5HK-Pm",
        "outputId": "f771240f-0cc7-4456-fadd-695fe3104906"
      },
      "source": [
        "# 在開始跑chat bot之前要在上面import的地方import model，以及上面的所有code都要跑過\n",
        "while True:\n",
        "  question = input('公威阿: ')\n",
        "  if question == '滾': # 輸入 '滾' 即可結束Chat Bot\n",
        "    print('...88')\n",
        "    break\n",
        "  ans_seq = ''\n",
        "  cur_token = start\n",
        "  word_count = 1\n",
        "  target_seq = np.zeros((1,A_max_length),dtype='int64')\n",
        "  target_seq[0,0] = A_word_to_index[start]\n",
        "  q = splitsentence(question)\n",
        "  q = fit_sentence(q[0],Q_word_to_index,Q_max_length)\n",
        "  while word_count < A_max_length:\n",
        "    decoder_output = model.predict([q,target_seq])\n",
        "    ind = np.argmax(decoder_output[0,word_count])\n",
        "    cur_token = A_index_to_word[ind]\n",
        "    if cur_token == end:\n",
        "      break\n",
        "    ans_seq += cur_token\n",
        "    target_seq[0,word_count] = ind\n",
        "    word_count += 1\n",
        "  print(ans_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "公威阿: 有沒有疫苗的掛?\n",
            " 你是不是不是嗎？\n",
            "公威阿: 你484確診了?\n",
            " 你是不是不是你的問題\n",
            "公威阿: 資工肥宅是防疫小尖兵嗎?\n",
            " 你是不是不是嗎？\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svcAZ8zKYT-W"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay9PzWDEOONA",
        "outputId": "1f528d69-836e-472a-dfba-75f1c95cdf9e"
      },
      "source": [
        "Q = '有沒有疫苗的掛?'\n",
        "Q = splitsentence(Q)\n",
        "print(Q)\n",
        "Q = fit_sentence(Q[0],Q_word_to_index,Q_max_length)\n",
        "A = fit_sentence([start],A_word_to_index,A_max_length)\n",
        "print(Q,A)\n",
        "pre = model.predict([Q,A])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['有', '沒', '有', '疫苗', '的', '掛', '?']]\n",
            "[[  6  14   6  20   2 174   4   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]] [[36397     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]]\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}